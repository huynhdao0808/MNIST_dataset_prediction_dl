{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MNIST dataset prediction by CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TJtYEDUdB1y",
        "colab_type": "text"
      },
      "source": [
        "# MNIST DATASET PREDICTION BY CNN\n",
        "Accuracy: 99.66%\n",
        "\n",
        "![link text](https://cdn-images-1.medium.com/max/600/1*2lSjt9YKJn9sxK7DSeGDyw.jpeg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2AArd9O68dE",
        "colab_type": "text"
      },
      "source": [
        "## Input library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqLjatzz5y_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras #this is quite useful to match our code with Keras documentation!\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTiTbsvRdW-g",
        "colab_type": "text"
      },
      "source": [
        "## PREPARATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9nd1KVO7JEs",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71XhtrEt5y_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full / 255.\n",
        "X_test = X_test / 255.\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkT8aDIi7Q4i",
        "colab_type": "text"
      },
      "source": [
        "### Image Augmentation Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpaIeM-LeWTh",
        "colab_type": "text"
      },
      "source": [
        "Normally, with other dataset, we use set of different kinds of augmention like:\n",
        "* Shifting\n",
        "* Flipping\n",
        "* Rotating\n",
        "* Shearing\n",
        "\n",
        "But in this case, some kinds of augmention cannot work, because it would lead to an unexpected change in our digits. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeSvxeO5y_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Image Augmentation\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,  \n",
        "    shear_range=0.15,\n",
        "    fill_mode=\"nearest\"\n",
        "    )\n",
        "\n",
        "image_data = image_generator.flow(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFcTk1Au7X7T",
        "colab_type": "text"
      },
      "source": [
        "### Callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUbnnovAgGb7",
        "colab_type": "text"
      },
      "source": [
        "Here I use patience = 10 for early stopping. Because sometimes, if the training result is too fluctuated and the patience is too small, it can lead to a too early stop training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ue01u-n5y_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,\n",
        "    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWskl_PI5y_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_callback = keras.callbacks.ModelCheckpoint(\n",
        "        filepath='mybestmodel.h5',\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptfAPpda5y_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, # Create ReduceLROnPlateau Callback\n",
        "                              patience=3, min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baJUKkpz7bwe",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaVXXePDkDtZ",
        "colab_type": "text"
      },
      "source": [
        "I prepare some other kinds of optimizer to replace Nadam.\n",
        "But normally, those will not work well if I donot fine tunning the hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hamUR18E5y_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_3HK6Pj5y_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXbR3QhE7hkK",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIDt4wc5y_2",
        "colab_type": "text"
      },
      "source": [
        "## Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "DvsRuaOV5y_3",
        "colab_type": "code",
        "outputId": "704243b0-9518-4b15-ec23-9c98b0dbec11",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback], validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1719 steps, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9273\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98820, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 17s 10ms/step - loss: 0.2414 - accuracy: 0.9275 - val_loss: 0.0479 - val_accuracy: 0.9882\n",
            "Epoch 2/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9658\n",
            "Epoch 00002: val_accuracy improved from 0.98820 to 0.98920, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1112 - accuracy: 0.9657 - val_loss: 0.0389 - val_accuracy: 0.9892\n",
            "Epoch 3/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0912 - accuracy: 0.9724\n",
            "Epoch 00003: val_accuracy improved from 0.98920 to 0.99120, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0911 - accuracy: 0.9724 - val_loss: 0.0347 - val_accuracy: 0.9912\n",
            "Epoch 4/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9766\n",
            "Epoch 00004: val_accuracy did not improve from 0.99120\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0779 - accuracy: 0.9766 - val_loss: 0.0349 - val_accuracy: 0.9904\n",
            "Epoch 5/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9798\n",
            "Epoch 00005: val_accuracy did not improve from 0.99120\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0685 - accuracy: 0.9798 - val_loss: 0.0328 - val_accuracy: 0.9910\n",
            "Epoch 6/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9802\n",
            "Epoch 00006: val_accuracy did not improve from 0.99120\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0635 - accuracy: 0.9802 - val_loss: 0.0383 - val_accuracy: 0.9906\n",
            "Epoch 7/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9816\n",
            "Epoch 00007: val_accuracy improved from 0.99120 to 0.99340, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0586 - accuracy: 0.9816 - val_loss: 0.0288 - val_accuracy: 0.9934\n",
            "Epoch 8/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9832\n",
            "Epoch 00008: val_accuracy did not improve from 0.99340\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0548 - accuracy: 0.9832 - val_loss: 0.0352 - val_accuracy: 0.9920\n",
            "Epoch 9/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9840\n",
            "Epoch 00009: val_accuracy did not improve from 0.99340\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0531 - accuracy: 0.9840 - val_loss: 0.0344 - val_accuracy: 0.9912\n",
            "Epoch 10/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9847\n",
            "Epoch 00010: val_accuracy did not improve from 0.99340\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0500 - accuracy: 0.9848 - val_loss: 0.0322 - val_accuracy: 0.9932\n",
            "Epoch 11/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9863\n",
            "Epoch 00011: val_accuracy did not improve from 0.99340\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0457 - accuracy: 0.9863 - val_loss: 0.0347 - val_accuracy: 0.9934\n",
            "Epoch 12/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9868\n",
            "Epoch 00012: val_accuracy did not improve from 0.99340\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0437 - accuracy: 0.9868 - val_loss: 0.0354 - val_accuracy: 0.9930\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97b5875110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywUYQ7rz5y_7",
        "colab_type": "code",
        "outputId": "ac462c4b-d36a-4611-a543-de6721b741c5",
        "colab": {}
      },
      "source": [
        "new_model = tf.keras.models.load_model('mybestmodel.h5')\n",
        "new_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
            "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0217 - accuracy: 0.9929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.021746549489399695, 0.9929]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uST2TTn5y_-",
        "colab_type": "text"
      },
      "source": [
        "## DA + Reduce learning rate on plateau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "XgrI3sNj5y__",
        "colab_type": "code",
        "outputId": "15d284e2-e899-47bd-acb0-0d0f351cd04f",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback,reduce_lr], validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1719 steps, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.9266\n",
            "Epoch 00001: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2396 - accuracy: 0.9268 - val_loss: 0.0494 - val_accuracy: 0.9886\n",
            "Epoch 2/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9655\n",
            "Epoch 00002: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1137 - accuracy: 0.9655 - val_loss: 0.0424 - val_accuracy: 0.9890\n",
            "Epoch 3/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9728\n",
            "Epoch 00003: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0931 - accuracy: 0.9729 - val_loss: 0.0359 - val_accuracy: 0.9886\n",
            "Epoch 4/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9757\n",
            "Epoch 00004: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0802 - accuracy: 0.9757 - val_loss: 0.0361 - val_accuracy: 0.9910\n",
            "Epoch 5/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9794\n",
            "Epoch 00005: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0698 - accuracy: 0.9794 - val_loss: 0.0349 - val_accuracy: 0.9914\n",
            "Epoch 6/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9815\n",
            "Epoch 00006: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0624 - accuracy: 0.9815 - val_loss: 0.0316 - val_accuracy: 0.9932\n",
            "Epoch 7/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9821\n",
            "Epoch 00007: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0591 - accuracy: 0.9821 - val_loss: 0.0356 - val_accuracy: 0.9922\n",
            "Epoch 8/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9834\n",
            "Epoch 00008: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0557 - accuracy: 0.9835 - val_loss: 0.0333 - val_accuracy: 0.9930\n",
            "Epoch 9/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9845\n",
            "Epoch 00009: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0512 - accuracy: 0.9845 - val_loss: 0.0441 - val_accuracy: 0.9912\n",
            "Epoch 10/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9883\n",
            "Epoch 00010: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0383 - accuracy: 0.9883 - val_loss: 0.0326 - val_accuracy: 0.9942\n",
            "Epoch 11/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0372 - accuracy: 0.9892\n",
            "Epoch 00011: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0371 - accuracy: 0.9892 - val_loss: 0.0299 - val_accuracy: 0.9946\n",
            "Epoch 12/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9891\n",
            "Epoch 00012: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0350 - accuracy: 0.9891 - val_loss: 0.0318 - val_accuracy: 0.9942\n",
            "Epoch 13/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0331 - accuracy: 0.9896\n",
            "Epoch 00013: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0355 - val_accuracy: 0.9936\n",
            "Epoch 14/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9901\n",
            "Epoch 00014: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0331 - val_accuracy: 0.9944\n",
            "Epoch 15/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9911\n",
            "Epoch 00015: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0282 - accuracy: 0.9911 - val_loss: 0.0321 - val_accuracy: 0.9946\n",
            "Epoch 16/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9925\n",
            "Epoch 00016: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.0371 - val_accuracy: 0.9942\n",
            "Epoch 17/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9925\n",
            "Epoch 00017: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.0355 - val_accuracy: 0.9946\n",
            "Epoch 18/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9937\n",
            "Epoch 00018: val_accuracy improved from 0.99480 to 0.99500, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0206 - accuracy: 0.9937 - val_loss: 0.0340 - val_accuracy: 0.9950\n",
            "Epoch 19/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9936\n",
            "Epoch 00019: val_accuracy improved from 0.99500 to 0.99540, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0324 - val_accuracy: 0.9954\n",
            "Epoch 20/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9933\n",
            "Epoch 00020: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0350 - val_accuracy: 0.9946\n",
            "Epoch 21/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9936\n",
            "Epoch 00021: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.0340 - val_accuracy: 0.9950\n",
            "Epoch 22/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9938\n",
            "Epoch 00022: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.0341 - val_accuracy: 0.9946\n",
            "Epoch 23/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9941\n",
            "Epoch 00023: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0181 - accuracy: 0.9941 - val_loss: 0.0350 - val_accuracy: 0.9950\n",
            "Epoch 24/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9945\n",
            "Epoch 00024: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.0352 - val_accuracy: 0.9950\n",
            "Epoch 25/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9939\n",
            "Epoch 00025: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.0341 - val_accuracy: 0.9952\n",
            "Epoch 26/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9944\n",
            "Epoch 00026: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.0343 - val_accuracy: 0.9948\n",
            "Epoch 27/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch 00027: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.0347 - val_accuracy: 0.9946\n",
            "Epoch 28/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
            "Epoch 00028: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0345 - val_accuracy: 0.9948\n",
            "Epoch 29/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9945\n",
            "Epoch 00029: val_accuracy did not improve from 0.99540\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0345 - val_accuracy: 0.9950\n",
            "Epoch 00029: early stopping\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0194 - accuracy: 0.9952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01938119462709324, 0.9952]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YotAGhWj5zAD",
        "colab_type": "code",
        "outputId": "d5789ee1-2c12-47a2-aa89-3385ea723d18",
        "colab": {}
      },
      "source": [
        "new_model = tf.keras.models.load_model('mybestmodel.h5')\n",
        "new_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
            "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0197 - accuracy: 0.9954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.019714194189903583, 0.9954]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azf92KGm5zAL",
        "colab_type": "text"
      },
      "source": [
        "## DA + ReduceLRonPlateau + 2 more Conv2D layers + 2 more Conv2D layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0GtZDltlqQq",
        "colab_type": "text"
      },
      "source": [
        "Whether I can make it better by adding 2 more Conv2D layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "DujHMnev5zAM",
        "colab_type": "code",
        "outputId": "6756f03d-7403-44f3-d354-d558e4166313",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback,reduce_lr], validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1719 steps, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9234\n",
            "Epoch 00001: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2457 - accuracy: 0.9234 - val_loss: 0.0393 - val_accuracy: 0.9882\n",
            "Epoch 2/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9750\n",
            "Epoch 00002: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0879 - accuracy: 0.9751 - val_loss: 0.0303 - val_accuracy: 0.9918\n",
            "Epoch 3/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9796\n",
            "Epoch 00003: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0709 - accuracy: 0.9796 - val_loss: 0.0239 - val_accuracy: 0.9934\n",
            "Epoch 4/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9823\n",
            "Epoch 00004: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0619 - accuracy: 0.9823 - val_loss: 0.0265 - val_accuracy: 0.9934\n",
            "Epoch 5/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9840\n",
            "Epoch 00005: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0546 - accuracy: 0.9840 - val_loss: 0.0324 - val_accuracy: 0.9930\n",
            "Epoch 6/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9858\n",
            "Epoch 00006: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0490 - accuracy: 0.9858 - val_loss: 0.0296 - val_accuracy: 0.9934\n",
            "Epoch 7/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9894\n",
            "Epoch 00007: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.0235 - val_accuracy: 0.9948\n",
            "Epoch 8/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9911\n",
            "Epoch 00008: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0314 - accuracy: 0.9911 - val_loss: 0.0241 - val_accuracy: 0.9938\n",
            "Epoch 9/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9920\n",
            "Epoch 00009: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0233 - val_accuracy: 0.9942\n",
            "Epoch 10/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0259 - accuracy: 0.9919\n",
            "Epoch 00010: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0260 - accuracy: 0.9919 - val_loss: 0.0216 - val_accuracy: 0.9940\n",
            "Epoch 11/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9939\n",
            "Epoch 00011: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.0272 - val_accuracy: 0.9950\n",
            "Epoch 12/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9945\n",
            "Epoch 00012: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0186 - accuracy: 0.9945 - val_loss: 0.0284 - val_accuracy: 0.9938\n",
            "Epoch 13/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9950\n",
            "Epoch 00013: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0174 - accuracy: 0.9950 - val_loss: 0.0289 - val_accuracy: 0.9950\n",
            "Epoch 14/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9950\n",
            "Epoch 00014: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0271 - val_accuracy: 0.9946\n",
            "Epoch 15/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9958\n",
            "Epoch 00015: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0281 - val_accuracy: 0.9940\n",
            "Epoch 16/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 00016: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.0299 - val_accuracy: 0.9942\n",
            "Epoch 17/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9968\n",
            "Epoch 00017: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.0260 - val_accuracy: 0.9942\n",
            "Epoch 18/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9968\n",
            "Epoch 00018: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.0263 - val_accuracy: 0.9954\n",
            "Epoch 19/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9968\n",
            "Epoch 00019: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0278 - val_accuracy: 0.9948\n",
            "Epoch 20/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9969\n",
            "Epoch 00020: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0263 - val_accuracy: 0.9946\n",
            "Epoch 21/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9974\n",
            "Epoch 00021: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0280 - val_accuracy: 0.9946\n",
            "Epoch 22/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9976\n",
            "Epoch 00022: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0290 - val_accuracy: 0.9944\n",
            "Epoch 23/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9973\n",
            "Epoch 00023: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0282 - val_accuracy: 0.9944\n",
            "Epoch 24/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9976\n",
            "Epoch 00024: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0286 - val_accuracy: 0.9950\n",
            "Epoch 25/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9973\n",
            "Epoch 00025: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.0286 - val_accuracy: 0.9948\n",
            "Epoch 26/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9977\n",
            "Epoch 00026: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0295 - val_accuracy: 0.9946\n",
            "Epoch 27/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974\n",
            "Epoch 00027: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0293 - val_accuracy: 0.9950\n",
            "Epoch 28/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9978\n",
            "Epoch 00028: val_accuracy did not improve from 0.99640\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0299 - val_accuracy: 0.9948\n",
            "Epoch 00028: early stopping\n",
            "10000/10000 [==============================] - 1s 69us/sample - loss: 0.0159 - accuracy: 0.9954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01589879503288201, 0.9954]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsH0ydUk5zAU",
        "colab_type": "text"
      },
      "source": [
        "## Add Dropout + L2 regularizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYv1pMi6l4pp",
        "colab_type": "text"
      },
      "source": [
        "Now the model become overfitting, I need to do something"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "EjGeUwqE5zAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback,reduce_lr], validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omcH_a_D5zAZ",
        "colab_type": "text"
      },
      "source": [
        "## Reduce the drop out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKJhYaSpmWOr",
        "colab_type": "text"
      },
      "source": [
        "Now it is underfitting, I need reduce the amount of drop out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "J0UGQjf-5zAZ",
        "colab_type": "code",
        "outputId": "d2a611fd-386a-463f-96c6-6ff9814ccf80",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback,reduce_lr], validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1719 steps, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.9248\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98900, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2578 - accuracy: 0.9250 - val_loss: 0.0575 - val_accuracy: 0.9890\n",
            "Epoch 2/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.1085 - accuracy: 0.9739\n",
            "Epoch 00002: val_accuracy improved from 0.98900 to 0.99080, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1084 - accuracy: 0.9739 - val_loss: 0.0574 - val_accuracy: 0.9908\n",
            "Epoch 3/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9793\n",
            "Epoch 00003: val_accuracy did not improve from 0.99080\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0921 - accuracy: 0.9794 - val_loss: 0.0540 - val_accuracy: 0.9906\n",
            "Epoch 4/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9812\n",
            "Epoch 00004: val_accuracy improved from 0.99080 to 0.99220, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0829 - accuracy: 0.9812 - val_loss: 0.0495 - val_accuracy: 0.9922\n",
            "Epoch 5/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9831\n",
            "Epoch 00005: val_accuracy did not improve from 0.99220\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0773 - accuracy: 0.9831 - val_loss: 0.0463 - val_accuracy: 0.9916\n",
            "Epoch 6/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9851\n",
            "Epoch 00006: val_accuracy did not improve from 0.99220\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0720 - accuracy: 0.9851 - val_loss: 0.0562 - val_accuracy: 0.9894\n",
            "Epoch 7/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9855\n",
            "Epoch 00007: val_accuracy improved from 0.99220 to 0.99320, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0695 - accuracy: 0.9855 - val_loss: 0.0489 - val_accuracy: 0.9932\n",
            "Epoch 8/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9853\n",
            "Epoch 00008: val_accuracy did not improve from 0.99320\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0675 - accuracy: 0.9853 - val_loss: 0.0481 - val_accuracy: 0.9932\n",
            "Epoch 9/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9867\n",
            "Epoch 00009: val_accuracy improved from 0.99320 to 0.99340, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0650 - accuracy: 0.9868 - val_loss: 0.0470 - val_accuracy: 0.9934\n",
            "Epoch 10/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9870\n",
            "Epoch 00010: val_accuracy improved from 0.99340 to 0.99480, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0634 - accuracy: 0.9870 - val_loss: 0.0441 - val_accuracy: 0.9948\n",
            "Epoch 11/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9879\n",
            "Epoch 00011: val_accuracy did not improve from 0.99480\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0603 - accuracy: 0.9879 - val_loss: 0.0478 - val_accuracy: 0.9902\n",
            "Epoch 12/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9875\n",
            "Epoch 00012: val_accuracy improved from 0.99480 to 0.99520, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0615 - accuracy: 0.9875 - val_loss: 0.0413 - val_accuracy: 0.9952\n",
            "Epoch 13/50\n",
            "1710/1719 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9880\n",
            "Epoch 00013: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0589 - accuracy: 0.9880 - val_loss: 0.0402 - val_accuracy: 0.9938\n",
            "Epoch 14/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9889\n",
            "Epoch 00014: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0571 - accuracy: 0.9889 - val_loss: 0.0393 - val_accuracy: 0.9938\n",
            "Epoch 15/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9889\n",
            "Epoch 00015: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0551 - accuracy: 0.9889 - val_loss: 0.0541 - val_accuracy: 0.9910\n",
            "Epoch 16/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 0.9921\n",
            "Epoch 00016: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0447 - accuracy: 0.9921 - val_loss: 0.0407 - val_accuracy: 0.9948\n",
            "Epoch 17/50\n",
            "1714/1719 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9924\n",
            "Epoch 00017: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0401 - accuracy: 0.9924 - val_loss: 0.0376 - val_accuracy: 0.9948\n",
            "Epoch 18/50\n",
            "1712/1719 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9922\n",
            "Epoch 00018: val_accuracy did not improve from 0.99520\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0388 - accuracy: 0.9923 - val_loss: 0.0314 - val_accuracy: 0.9950\n",
            "Epoch 19/50\n",
            "1716/1719 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9940\n",
            "Epoch 00019: val_accuracy improved from 0.99520 to 0.99560, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0305 - accuracy: 0.9940 - val_loss: 0.0310 - val_accuracy: 0.9956\n",
            "Epoch 20/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9946\n",
            "Epoch 00020: val_accuracy did not improve from 0.99560\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0272 - accuracy: 0.9946 - val_loss: 0.0304 - val_accuracy: 0.9952\n",
            "Epoch 21/50\n",
            "1713/1719 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.9948\n",
            "Epoch 00021: val_accuracy improved from 0.99560 to 0.99680, saving model to mybestmodel.h5\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0264 - accuracy: 0.9949 - val_loss: 0.0284 - val_accuracy: 0.9968\n",
            "Epoch 22/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9951\n",
            "Epoch 00022: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0252 - accuracy: 0.9951 - val_loss: 0.0307 - val_accuracy: 0.9956\n",
            "Epoch 23/50\n",
            "1717/1719 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9947\n",
            "Epoch 00023: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0253 - accuracy: 0.9947 - val_loss: 0.0266 - val_accuracy: 0.9960\n",
            "Epoch 24/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9952\n",
            "Epoch 00024: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0232 - accuracy: 0.9952 - val_loss: 0.0289 - val_accuracy: 0.9964\n",
            "Epoch 25/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9958\n",
            "Epoch 00025: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0212 - accuracy: 0.9958 - val_loss: 0.0261 - val_accuracy: 0.9964\n",
            "Epoch 26/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9960\n",
            "Epoch 00026: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.0260 - val_accuracy: 0.9964\n",
            "Epoch 27/50\n",
            "1711/1719 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9967\n",
            "Epoch 00027: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0179 - accuracy: 0.9967 - val_loss: 0.0273 - val_accuracy: 0.9958\n",
            "Epoch 28/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9960\n",
            "Epoch 00028: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0187 - accuracy: 0.9960 - val_loss: 0.0257 - val_accuracy: 0.9966\n",
            "Epoch 29/50\n",
            "1718/1719 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9966\n",
            "Epoch 00029: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0174 - accuracy: 0.9966 - val_loss: 0.0264 - val_accuracy: 0.9964\n",
            "Epoch 30/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9967\n",
            "Epoch 00030: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0168 - accuracy: 0.9967 - val_loss: 0.0272 - val_accuracy: 0.9958\n",
            "Epoch 31/50\n",
            "1715/1719 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9963\n",
            "Epoch 00031: val_accuracy did not improve from 0.99680\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0180 - accuracy: 0.9963 - val_loss: 0.0253 - val_accuracy: 0.9964\n",
            "Epoch 00031: early stopping\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 0.0187 - accuracy: 0.9966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.018727440229803324, 0.9966]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsalLyc5zAc",
        "colab_type": "code",
        "outputId": "82b2a817-435d-4edc-ec2b-7019be5635bc",
        "colab": {}
      },
      "source": [
        "new_model = tf.keras.models.load_model('mynewbestmodel.h5')\n",
        "new_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: 0.0187 - accuracy: 0.9966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.018727440229803324, 0.9966]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGMu2BDJb4Ld",
        "colab_type": "text"
      },
      "source": [
        "## Another approach with Batch Normalization and Global Average Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uebhK2BAmefr",
        "colab_type": "text"
      },
      "source": [
        "* Because the model now is too deep, we can add BatchNormalization between each block to prevent the disapeared as well as explosed gradient situation\n",
        "* And at the end of Conv block we can use the Global Average Pooling, it is the common practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV0n_uMrb2-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization()\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    karas.layers.GlobalAveragePooling2D()\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(image_data, epochs=50,callbacks=[es_callback,cp_callback,reduce_lr], validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}